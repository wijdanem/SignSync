# -*- coding: utf-8 -*-
"""Signsync.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NQ3vaVPIgMouvCNox5eZduU9I_nLQkFd

The purpose of this file:
CNN model for recognizing ASL gesures.
contains: required libraries, buiding the cnn model, compiling the model, data augmentation and preprocessing, leadoing the dataset, training the model.
"""

import numpy as np
import os
import cv2

# Importing required modules from Keras
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.utils import Sequence

#initializing the model
model = Sequential()

# First block of convolutional layers with pooling and dropout
model.add(Conv2D(filters=32, kernel_size=(3, 3), padding='same', activation='relu', input_shape=(64, 64, 3)))
model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(rate=0.25))

# Second block with increased filters
model.add(Conv2D(filters=64, kernel_size=(3, 3), padding='same', activation='relu'))
model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(rate=0.25))

# Third block for deeper feature extraction
model.add(Conv2D(filters=64, kernel_size=(3, 3), padding='same', activation='relu'))
model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(rate=0.25))

# Flattening the feature maps
model.add(Flatten())

# Fully connected layer for classification
model.add(Dense(units=512, activation='relu'))
model.add(Dropout(rate=0.5))
model.add(Dense(units=12, activation='softmax'))

# Compiling the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Print the model summary to verify its structure
print(model.summary())

!pip install --upgrade keras

!pip install --upgrade tensorflow

from google.colab import drive
drive.mount('/content/drive')

# Custom video generator function
def video_generator(directory, target_size=(64, 64), batch_size=32, classes=None):
    while True:
        for class_name in classes:
            class_dir = os.path.join(directory, class_name)
            for video_name in os.listdir(class_dir):
                video_path = os.path.join(class_dir, video_name)
                # Read the video
                cap = cv2.VideoCapture(video_path)
                while cap.isOpened():
                    ret, frame = cap.read()
                    if not ret:
                        break
                    frame = cv2.resize(frame, target_size)
                    frame = frame.astype('float32') / 255.0
                    # Yield the frame and corresponding label
                    yield np.expand_dims(frame, axis=0), class_name
                cap.release()

# Define the path to the training data
train_dir = '/content/drive/MyDrive/MyDatasets/dataset/train'
classes = os.listdir(train_dir)  # Get the class names from the train directory

# Create the training data generator
train_gen = video_generator(train_dir, classes=classes)

# Example to fetch a batch of data
X_batch, y_batch = next(train_gen)

# Print the shapes of the fetched data
print(f"X_batch shape: {X_batch.shape}")
print(f"y_batch: {y_batch}")

# Optionally, print the first frame in the batc
import matplotlib.pyplot as plt

plt.imshow(X_batch[0])
plt.title(f"Label: {y_batch}")
plt.axis('off')  # Turn off axis numbers and ticks
plt.show()

cclass VideoDataGenerator(Sequence):
    def __init__(self, video_folder, labels, batch_size=32, img_size=(64, 64), max_frames=30):
        self.video_folder = video_folder
        self.labels = labels
        self.batch_size = batch_size
        self.img_size = img_size
        self.max_frames = max_frames
        self.video_files = self._get_video_files()  # Call to the method here

def _get_video_files(self):
        """Retrieve a list of video files along with their corresponding labels."""
        files = []
        for label in self.labels:
            label_path = os.path.join(self.video_folder, label)
            # Check if the label path is a directory
            if os.path.isdir(label_path):
                for video in os.listdir(label_path):
                    if video.endswith('.mp4'):
                        files.append((os.path.join(label_path, video), label))
        return files

def __len__(self):
        return int(np.ceil(len(self.video_files) / self.batch_size))

def __getitem__(self, idx):
        batch_files = self.video_files[idx * self.batch_size:(idx + 1) * self.batch_size]
        X, y = [], []
        for file, label in batch_files:
            cap = cv2.VideoCapture(file)
            frames = []
            while cap.isOpened():
                ret, frame = cap.read()
                if not ret or len(frames) >= self.max_frames:
                    break
                frame = cv2.resize(frame, self.img_size)
                frames.append(frame)
            cap.release()

            # Pad frames if fewer than max_frames
            while len(frames) < self.max_frames:
                frames.append(np.zeros((*self.img_size, 3)))

            X.append(np.array(frames))
            y.append(self.labels.index(label))  # Convert label to index

        return np.array(X), np.array(y)

# Example usage
video_folder = '/content/drive/MyDrive/MyDatasets/dataset/train'
labels = os.listdir(video_folder)
video_gen = VideoDataGenerator(video_folder, labels)

# Fetch a batch of videos
X_batch, y_batch = next(iter(video_gen))